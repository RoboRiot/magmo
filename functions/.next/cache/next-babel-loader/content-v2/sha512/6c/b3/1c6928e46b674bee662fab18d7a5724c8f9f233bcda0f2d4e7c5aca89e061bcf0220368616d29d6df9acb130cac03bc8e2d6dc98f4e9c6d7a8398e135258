{"ast":null,"code":"import fs from \"fs\";\nimport path from \"path\";\nimport { OpenAI } from \"openai\";\nimport cosineSim from \"../../../utils/cosineSim\";\nimport { requireFirebaseAuth } from \"../../../utils/apiAuth\"; // how many best chunks to send to the model\n\nconst TOP_K = 5; // cache store in memory so we don't read disk every request\n\nlet STORE_CACHE = null;\n\nfunction loadStore() {\n  if (STORE_CACHE) return STORE_CACHE; // you're storing knowledge_store.json in the same folder as this route:\n  // pages/api/gpt/knowledge_store.json\n\n  const storePath = path.join(process.cwd(), \"pages\", \"api\", \"gpt\", \"knowledge_store.json\");\n  const raw = fs.readFileSync(storePath, \"utf-8\");\n  STORE_CACHE = JSON.parse(raw); // [{ source, text, embedding: [ ...numbers ] }, ...]\n\n  return STORE_CACHE;\n}\n\nasync function embedQuestion(client, question) {\n  const resp = await client.embeddings.create({\n    model: \"text-embedding-3-large\",\n    input: [question]\n  });\n  return resp.data[0].embedding;\n}\n\nfunction buildContextFromChunks(chunks) {\n  return chunks.map((c, i) => `Source: ${c.source}\\nContent:\\n${c.text}`).join(\"\\n\\n---\\n\\n\");\n}\n\nexport default async function handler(req, res) {\n  if (req.method !== \"POST\") {\n    return res.status(405).json({\n      error: \"Method not allowed. Send POST with { question }.\"\n    });\n  }\n\n  try {\n    await requireFirebaseAuth(req, res);\n    if (res.writableEnded) return;\n    const {\n      question\n    } = req.body || {};\n\n    if (!question || typeof question !== \"string\") {\n      return res.status(400).json({\n        error: \"Missing or invalid 'question' in body.\"\n      });\n    } // init OpenAI client with your server-side key\n\n\n    const client = new OpenAI({\n      apiKey: process.env.OPENAI_API_KEY\n    }); // 1. load all chunks from the local knowledge store\n\n    const store = loadStore(); // array of { source, text, embedding }\n    // 2. embed the user's question\n\n    const qVec = await embedQuestion(client, question); // 3. score each chunk by cosine similarity\n\n    const scored = store.map(chunk => {\n      const sim = cosineSim(qVec, chunk.embedding);\n      return {\n        sim,\n        chunk\n      };\n    }); // 4. take the top K most relevant chunks\n\n    scored.sort((a, b) => b.sim - a.sim);\n    const topChunks = scored.slice(0, TOP_K).map(x => x.chunk); // 5. build a context block from those chunks\n\n    const contextText = buildContextFromChunks(topChunks); // 6. ask the model to answer USING ONLY THAT CONTEXT\n\n    const completion = await client.responses.create({\n      model: \"gpt-5\",\n      // or \"gpt-4o\" etc. use the model you have quota for\n      input: [{\n        role: \"system\",\n        content: \"You are Magmo, an internal AIS service assistant for MRI/CT installs, faults, and safety. \" + \"You must answer using ONLY the provided context below. \" + \"If you are not certain or it is safety critical, say you are not certain and advise escalation. \" + \"Speak in direct technician language, not marketing language.\"\n      }, {\n        role: \"user\",\n        content: `Question: ${question}\\n\\n` + `Relevant manual context:\\n${contextText}`\n      }]\n    });\n    const finalAnswer = completion.output_text || \"(No answer)\";\n    return res.status(200).json({\n      answer: finalAnswer\n    });\n  } catch (err) {\n    console.error(\"AskMagmo API error:\", err);\n    return res.status(500).json({\n      error: \"Internal Server Error while answering from manuals.\"\n    });\n  }\n}","map":null,"metadata":{},"sourceType":"module"}