{"ast":null,"code":"import fs from \"fs\";\nimport path from \"path\";\nimport { OpenAI } from \"openai\";\nimport cosineSim from \"../../../utils/cosineSim\"; // how many best chunks to send to the model\n\nconst TOP_K = 5; // cache store in memory so we don't read disk every request\n\nlet STORE_CACHE = null;\n\nfunction loadStore() {\n  if (STORE_CACHE) return STORE_CACHE; // you're storing knowledge_store.json in the same folder as this route:\n  // pages/api/gpt/knowledge_store.json\n\n  const storePath = path.join(process.cwd(), \"pages\", \"api\", \"gpt\", \"knowledge_store.json\");\n  const raw = fs.readFileSync(storePath, \"utf-8\");\n  STORE_CACHE = JSON.parse(raw); // [{ source, text, embedding: [ ...numbers ] }, ...]\n\n  return STORE_CACHE;\n}\n\nasync function embedQuestion(client, question) {\n  const resp = await client.embeddings.create({\n    model: \"text-embedding-3-large\",\n    input: [question]\n  });\n  return resp.data[0].embedding;\n}\n\nfunction buildContextFromChunks(chunks) {\n  return chunks.map((c, i) => `Source: ${c.source}\\nContent:\\n${c.text}`).join(\"\\n\\n---\\n\\n\");\n}\n\nexport default async function handler(req, res) {\n  if (req.method !== \"POST\") {\n    return res.status(405).json({\n      error: \"Method not allowed. Send POST with { question }.\"\n    });\n  }\n\n  try {\n    const {\n      question\n    } = req.body || {};\n\n    if (!question || typeof question !== \"string\") {\n      return res.status(400).json({\n        error: \"Missing or invalid 'question' in body.\"\n      });\n    } // init OpenAI client with your server-side key\n\n\n    const client = new OpenAI({\n      apiKey: process.env.OPENAI_API_KEY\n    }); // 1. load all chunks from the local knowledge store\n\n    const store = loadStore(); // array of { source, text, embedding }\n    // 2. embed the user's question\n\n    const qVec = await embedQuestion(client, question); // 3. score each chunk by cosine similarity\n\n    const scored = store.map(chunk => {\n      const sim = cosineSim(qVec, chunk.embedding);\n      return {\n        sim,\n        chunk\n      };\n    }); // 4. take the top K most relevant chunks\n\n    scored.sort((a, b) => b.sim - a.sim);\n    const topChunks = scored.slice(0, TOP_K).map(x => x.chunk); // 5. build a context block from those chunks\n\n    const contextText = buildContextFromChunks(topChunks); // 6. ask the model to answer USING ONLY THAT CONTEXT\n\n    const completion = await client.responses.create({\n      model: \"gpt-5\",\n      // or \"gpt-4o\" etc. use the model you have quota for\n      input: [{\n        role: \"system\",\n        content: \"You are Magmo, an internal AIS service assistant for MRI/CT installs, faults, and safety. \" + \"You must answer using ONLY the provided context below. \" + \"If you are not certain or it is safety critical, say you are not certain and advise escalation. \" + \"Speak in direct technician language, not marketing language.\"\n      }, {\n        role: \"user\",\n        content: `Question: ${question}\\n\\n` + `Relevant manual context:\\n${contextText}`\n      }]\n    });\n    const finalAnswer = completion.output_text || \"(No answer)\";\n    return res.status(200).json({\n      answer: finalAnswer\n    });\n  } catch (err) {\n    console.error(\"AskMagmo API error:\", err);\n    return res.status(500).json({\n      error: \"Internal Server Error while answering from manuals.\"\n    });\n  }\n}","map":{"version":3,"sources":["C:/Users/mack2/OneDrive/Desktop/code/pages/api/gpt/AskMagmo.js"],"names":["fs","path","OpenAI","cosineSim","TOP_K","STORE_CACHE","loadStore","storePath","join","process","cwd","raw","readFileSync","JSON","parse","embedQuestion","client","question","resp","embeddings","create","model","input","data","embedding","buildContextFromChunks","chunks","map","c","i","source","text","handler","req","res","method","status","json","error","body","apiKey","env","OPENAI_API_KEY","store","qVec","scored","chunk","sim","sort","a","b","topChunks","slice","x","contextText","completion","responses","role","content","finalAnswer","output_text","answer","err","console"],"mappings":"AAAA,OAAOA,EAAP,MAAe,IAAf;AACA,OAAOC,IAAP,MAAiB,MAAjB;AACA,SAASC,MAAT,QAAuB,QAAvB;AACA,OAAOC,SAAP,MAAsB,0BAAtB,C,CAEA;;AACA,MAAMC,KAAK,GAAG,CAAd,C,CAEA;;AACA,IAAIC,WAAW,GAAG,IAAlB;;AAEA,SAASC,SAAT,GAAqB;AACnB,MAAID,WAAJ,EAAiB,OAAOA,WAAP,CADE,CAGnB;AACA;;AACA,QAAME,SAAS,GAAGN,IAAI,CAACO,IAAL,CAAUC,OAAO,CAACC,GAAR,EAAV,EAAyB,OAAzB,EAAkC,KAAlC,EAAyC,KAAzC,EAAgD,sBAAhD,CAAlB;AAEA,QAAMC,GAAG,GAAGX,EAAE,CAACY,YAAH,CAAgBL,SAAhB,EAA2B,OAA3B,CAAZ;AACAF,EAAAA,WAAW,GAAGQ,IAAI,CAACC,KAAL,CAAWH,GAAX,CAAd,CARmB,CAQY;;AAC/B,SAAON,WAAP;AACD;;AAED,eAAeU,aAAf,CAA6BC,MAA7B,EAAqCC,QAArC,EAA+C;AAC7C,QAAMC,IAAI,GAAG,MAAMF,MAAM,CAACG,UAAP,CAAkBC,MAAlB,CAAyB;AAC1CC,IAAAA,KAAK,EAAE,wBADmC;AAE1CC,IAAAA,KAAK,EAAE,CAACL,QAAD;AAFmC,GAAzB,CAAnB;AAIA,SAAOC,IAAI,CAACK,IAAL,CAAU,CAAV,EAAaC,SAApB;AACD;;AAED,SAASC,sBAAT,CAAgCC,MAAhC,EAAwC;AACtC,SAAOA,MAAM,CACVC,GADI,CAEH,CAACC,CAAD,EAAIC,CAAJ,KACG,WAAUD,CAAC,CAACE,MAAO,eAAcF,CAAC,CAACG,IAAK,EAHxC,EAKJvB,IALI,CAKC,aALD,CAAP;AAMD;;AAED,eAAe,eAAewB,OAAf,CAAuBC,GAAvB,EAA4BC,GAA5B,EAAiC;AAC9C,MAAID,GAAG,CAACE,MAAJ,KAAe,MAAnB,EAA2B;AACzB,WAAOD,GAAG,CACPE,MADI,CACG,GADH,EAEJC,IAFI,CAEC;AAAEC,MAAAA,KAAK,EAAE;AAAT,KAFD,CAAP;AAGD;;AAED,MAAI;AACF,UAAM;AAAErB,MAAAA;AAAF,QAAegB,GAAG,CAACM,IAAJ,IAAY,EAAjC;;AAEA,QAAI,CAACtB,QAAD,IAAa,OAAOA,QAAP,KAAoB,QAArC,EAA+C;AAC7C,aAAOiB,GAAG,CACPE,MADI,CACG,GADH,EAEJC,IAFI,CAEC;AAAEC,QAAAA,KAAK,EAAE;AAAT,OAFD,CAAP;AAGD,KAPC,CASF;;;AACA,UAAMtB,MAAM,GAAG,IAAId,MAAJ,CAAW;AACxBsC,MAAAA,MAAM,EAAE/B,OAAO,CAACgC,GAAR,CAAYC;AADI,KAAX,CAAf,CAVE,CAcF;;AACA,UAAMC,KAAK,GAAGrC,SAAS,EAAvB,CAfE,CAeyB;AAE3B;;AACA,UAAMsC,IAAI,GAAG,MAAM7B,aAAa,CAACC,MAAD,EAASC,QAAT,CAAhC,CAlBE,CAoBF;;AACA,UAAM4B,MAAM,GAAGF,KAAK,CAAChB,GAAN,CAAWmB,KAAD,IAAW;AAClC,YAAMC,GAAG,GAAG5C,SAAS,CAACyC,IAAD,EAAOE,KAAK,CAACtB,SAAb,CAArB;AACA,aAAO;AAAEuB,QAAAA,GAAF;AAAOD,QAAAA;AAAP,OAAP;AACD,KAHc,CAAf,CArBE,CA0BF;;AACAD,IAAAA,MAAM,CAACG,IAAP,CAAY,CAACC,CAAD,EAAIC,CAAJ,KAAUA,CAAC,CAACH,GAAF,GAAQE,CAAC,CAACF,GAAhC;AACA,UAAMI,SAAS,GAAGN,MAAM,CAACO,KAAP,CAAa,CAAb,EAAgBhD,KAAhB,EAAuBuB,GAAvB,CAA4B0B,CAAD,IAAOA,CAAC,CAACP,KAApC,CAAlB,CA5BE,CA8BF;;AACA,UAAMQ,WAAW,GAAG7B,sBAAsB,CAAC0B,SAAD,CAA1C,CA/BE,CAiCF;;AACA,UAAMI,UAAU,GAAG,MAAMvC,MAAM,CAACwC,SAAP,CAAiBpC,MAAjB,CAAwB;AAC/CC,MAAAA,KAAK,EAAE,OADwC;AAC/B;AAChBC,MAAAA,KAAK,EAAE,CACL;AACEmC,QAAAA,IAAI,EAAE,QADR;AAEEC,QAAAA,OAAO,EACL,+FACA,yDADA,GAEA,kGAFA,GAGA;AANJ,OADK,EASL;AACED,QAAAA,IAAI,EAAE,MADR;AAEEC,QAAAA,OAAO,EACJ,aAAYzC,QAAS,MAAtB,GACC,6BAA4BqC,WAAY;AAJ7C,OATK;AAFwC,KAAxB,CAAzB;AAoBA,UAAMK,WAAW,GAAGJ,UAAU,CAACK,WAAX,IAA0B,aAA9C;AAEA,WAAO1B,GAAG,CAACE,MAAJ,CAAW,GAAX,EAAgBC,IAAhB,CAAqB;AAC1BwB,MAAAA,MAAM,EAAEF;AADkB,KAArB,CAAP;AAGD,GA3DD,CA2DE,OAAOG,GAAP,EAAY;AACZC,IAAAA,OAAO,CAACzB,KAAR,CAAc,qBAAd,EAAqCwB,GAArC;AACA,WAAO5B,GAAG,CAACE,MAAJ,CAAW,GAAX,EAAgBC,IAAhB,CAAqB;AAC1BC,MAAAA,KAAK,EAAE;AADmB,KAArB,CAAP;AAGD;AACF","sourcesContent":["import fs from \"fs\";\r\nimport path from \"path\";\r\nimport { OpenAI } from \"openai\";\r\nimport cosineSim from \"../../../utils/cosineSim\";\r\n\r\n// how many best chunks to send to the model\r\nconst TOP_K = 5;\r\n\r\n// cache store in memory so we don't read disk every request\r\nlet STORE_CACHE = null;\r\n\r\nfunction loadStore() {\r\n  if (STORE_CACHE) return STORE_CACHE;\r\n\r\n  // you're storing knowledge_store.json in the same folder as this route:\r\n  // pages/api/gpt/knowledge_store.json\r\n  const storePath = path.join(process.cwd(), \"pages\", \"api\", \"gpt\", \"knowledge_store.json\");\r\n\r\n  const raw = fs.readFileSync(storePath, \"utf-8\");\r\n  STORE_CACHE = JSON.parse(raw); // [{ source, text, embedding: [ ...numbers ] }, ...]\r\n  return STORE_CACHE;\r\n}\r\n\r\nasync function embedQuestion(client, question) {\r\n  const resp = await client.embeddings.create({\r\n    model: \"text-embedding-3-large\",\r\n    input: [question],\r\n  });\r\n  return resp.data[0].embedding;\r\n}\r\n\r\nfunction buildContextFromChunks(chunks) {\r\n  return chunks\r\n    .map(\r\n      (c, i) =>\r\n        `Source: ${c.source}\\nContent:\\n${c.text}`\r\n    )\r\n    .join(\"\\n\\n---\\n\\n\");\r\n}\r\n\r\nexport default async function handler(req, res) {\r\n  if (req.method !== \"POST\") {\r\n    return res\r\n      .status(405)\r\n      .json({ error: \"Method not allowed. Send POST with { question }.\" });\r\n  }\r\n\r\n  try {\r\n    const { question } = req.body || {};\r\n\r\n    if (!question || typeof question !== \"string\") {\r\n      return res\r\n        .status(400)\r\n        .json({ error: \"Missing or invalid 'question' in body.\" });\r\n    }\r\n\r\n    // init OpenAI client with your server-side key\r\n    const client = new OpenAI({\r\n      apiKey: process.env.OPENAI_API_KEY,\r\n    });\r\n\r\n    // 1. load all chunks from the local knowledge store\r\n    const store = loadStore(); // array of { source, text, embedding }\r\n\r\n    // 2. embed the user's question\r\n    const qVec = await embedQuestion(client, question);\r\n\r\n    // 3. score each chunk by cosine similarity\r\n    const scored = store.map((chunk) => {\r\n      const sim = cosineSim(qVec, chunk.embedding);\r\n      return { sim, chunk };\r\n    });\r\n\r\n    // 4. take the top K most relevant chunks\r\n    scored.sort((a, b) => b.sim - a.sim);\r\n    const topChunks = scored.slice(0, TOP_K).map((x) => x.chunk);\r\n\r\n    // 5. build a context block from those chunks\r\n    const contextText = buildContextFromChunks(topChunks);\r\n\r\n    // 6. ask the model to answer USING ONLY THAT CONTEXT\r\n    const completion = await client.responses.create({\r\n      model: \"gpt-5\", // or \"gpt-4o\" etc. use the model you have quota for\r\n      input: [\r\n        {\r\n          role: \"system\",\r\n          content:\r\n            \"You are Magmo, an internal AIS service assistant for MRI/CT installs, faults, and safety. \" +\r\n            \"You must answer using ONLY the provided context below. \" +\r\n            \"If you are not certain or it is safety critical, say you are not certain and advise escalation. \" +\r\n            \"Speak in direct technician language, not marketing language.\",\r\n        },\r\n        {\r\n          role: \"user\",\r\n          content:\r\n            `Question: ${question}\\n\\n` +\r\n            `Relevant manual context:\\n${contextText}`,\r\n        },\r\n      ],\r\n    });\r\n\r\n    const finalAnswer = completion.output_text || \"(No answer)\";\r\n\r\n    return res.status(200).json({\r\n      answer: finalAnswer,\r\n    });\r\n  } catch (err) {\r\n    console.error(\"AskMagmo API error:\", err);\r\n    return res.status(500).json({\r\n      error: \"Internal Server Error while answering from manuals.\",\r\n    });\r\n  }\r\n}\r\n"]},"metadata":{},"sourceType":"module"}